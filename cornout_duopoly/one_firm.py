import random
from typing import Optional
import numpy as np
import gymnasium as gym

class OneDuopoly(gym.Env):
    """
    Gym environment for a simple duopoly scenario.
    In this environment, two firms are competing in a market by choosing production quantities.
    The reward is based on the profit generated by both firms.
    """

    def __init__(self, a=300, b=1, cost=30):
        """
        Initialize the duopoly environment.

        Parameters:
        - a: the demand curve intercept (default 300)
        - b: the slope of the demand curve (default 1)
        - cost: the cost of producing one unit of the good (default 30)
        """
        # Initialize variables for the environment
        self.a = a
        self.b = b
        self.cost = cost

        # Quantities and profits for both firms
        self._quantity1 = 0
        self._quantity2 = 0
        self._profit1 = 0
        self._profit2 = 0

        # Maximum production possible for both firms
        self._max_production = int(2 * (self.a - self.cost) / 3)

        # Maximum possible profit for Firm 1 (based on max production)
        self._max_profit = int(self.a - self.cost - b * self._max_production) * self._max_production

        # Define the action and observation spaces
        self.observation_space = gym.spaces.Discrete(self._max_production + 1)  # Firm 1 can produce up to _max_production units
        self.action_space = gym.spaces.Discrete(n=101, start=-50)  # Action space: firm can adjust production by [-50, 50] units

    def step(self, action):
        """
        Take one step in the environment by adjusting Firm 1's production.
        
        Parameters:
        - action: the amount by which Firm 1 changes its production quantity

        Returns:
        - observation: the new state of Firm 1's production quantity
        - reward: the combined profit of both firms
        - terminated: whether the episode is over
        - truncated: whether the episode was cut short
        - info: additional information (empty dictionary in this case)
        """
        # Adjust Firm 1's quantity based on the action taken
        self._quantity1 = np.clip(self._quantity1 + action, 0, self._max_production)

        # Calculate Firm 1's profit using the demand curve and quantity produced by both firms
        self._profit1 = (self.a - self.cost - self.b * (self._quantity1 + self._quantity2)) * self._quantity1

        # Calculate Firm 2's optimal quantity (assuming Firm 2 is optimizing profit)
        self._quantity2 = (self.a - self.cost - self.b * self._quantity1) / (2 * self.b)

        # Calculate Firm 2's profit
        self._profit2 = (self.a - self.cost - self.b * (self._quantity1 + self._quantity2)) * self._quantity2

        # Reward is the combined profit of both firms
        reward = self._profit1 + self._profit2

        # Termination condition: Episode ends when quantities are close enough to each other and Firm 1 has a non-trivial production quantity
        terminated = (abs(self._quantity1 - self._quantity2) < 2) and self._quantity1 > 20
        truncated = False

        # Get the current state and additional info for the episode
        observation = self._get_obs()
        info = self._get_info()

        return observation, reward, terminated, truncated, info
    
    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):
        """
        Reset the environment to its initial state for a new episode.

        Parameters:
        - seed: optional seed for randomness
        - options: additional options for resetting the environment

        Returns:
        - observation: the initial state of Firm 1's production quantity
        - info: additional information (empty dictionary in this case)
        """
        # Reset the quantities and profits for both firms
        super().reset(seed=seed)
        self._quantity1 = 0
        self._profit1 = 0
        self._quantity2 = 0
        self._profit2 = 0

        # Return initial observation and info
        observation = self._get_obs()
        info = self._get_info()

        return observation, info
    
    def _get_obs(self):
        """
        Get the current observation for the environment.
        
        Returns:
        - current quantity produced by Firm 1
        """
        return self._quantity1
    
    def _get_info(self):
        """
        Get additional information about the environment (in this case, an empty dictionary).
        
        Returns:
        - Empty dictionary
        """
        return {}
    
    def render(self):
        """
        Render the current state of the environment by printing the quantities and profits of both firms.
        """
        print(f"Firm 1 Quantity - profit: {self._quantity1} - {self._profit1}, Firm 2 Quantity - profit: {self._quantity2} - {self._profit2}")


# Register the environment in Gymnasium for use
gym.register(
    id="gymnassium_evnv/Duopoly-v1",
    entry_point=OneDuopoly,
)

# Create the environment instance
env = gym.make("gymnassium_evnv/Duopoly-v1")

# Initialize Q-table (state-action values) for the Q-learning algorithm
q_table = np.zeros((env.observation_space.n, env.action_space.n))

# Hyperparameters for Q-learning
learning_rate = 0.1  # Learning rate for Q-table updates
discount = 0.99      # Discount factor for future rewards
episodes = 100       # Number of training episodes
epsilon = 0.1        # Exploration rate for epsilon-greedy policy

# List to store rewards for each episode
episode_rewards = []

# Training loop for Q-learning
for episode in range(episodes):
    observation, info = env.reset()  # Reset the environment for each episode
    state = observation  # Initial state
    episode_over = False  # Flag to indicate if the episode is over
    total_episode_reward = 0  # Total reward accumulated in the episode
    
    while not episode_over:
        # Epsilon-greedy policy: choose action based on exploration (random) or exploitation (best action)
        if random.uniform(0, 1) < epsilon:  # Explore
            action = env.action_space.sample()
        else:  # Exploit
            action = np.argmax(q_table[state])  # Choose the action with the highest Q-value

        # Execute the chosen action and observe the new state and reward
        observation, reward, terminated, truncated, info = env.step(action)
        new_state = observation  # Update the state

        # If the episode is not terminated, update the Q-table
        if not terminated:
            max_future_q = np.max(q_table[new_state])  # Max Q-value for the next state
            current_q = q_table[state, action]  # Current Q-value for the state-action pair

            # Q-learning update rule
            new_q = current_q + learning_rate * (reward + discount * max_future_q - current_q)
            q_table[state, action] = new_q  # Update the Q-table with the new Q-value

        state = new_state  # Move to the next state

        # Check if the episode is over (either terminated or truncated)
        episode_over = terminated or truncated
        total_episode_reward += reward  # Accumulate the total reward for the episode

    # Render the environment state (optional visualization)
    env.render()

    # Store the total reward for this episode
    episode_rewards.append(total_episode_reward)
    print(f"Episode {episode}: Episode Reward: {total_episode_reward}")

# Calculate and print the average reward over all episodes
average_reward = sum(episode_rewards) / len(episode_rewards)
print(f"Average Reward over {episodes} episodes: {average_reward}")
